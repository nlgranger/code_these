{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "\n",
    "os.environ['THEANO_FLAGS'] = \"device=cuda1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shelve\n",
    "from bisect import bisect\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "import seqtools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sltools.nn_utils import adjust_length\n",
    "\n",
    "from experiments.siamese_triplet.a_data import \\\n",
    "    durations, labels, transformations, \\\n",
    "    train_subset, val_subset\n",
    "from experiments.siamese_triplet.b_preprocess import skel_feat_seqs\n",
    "\n",
    "np.set_printoptions(linewidth=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.siamese_triplet.a_data import cachedir\n",
    "\n",
    "report = shelve.open(os.path.join(cachedir, \"rnn_report\"))\n",
    "report.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_time = 128\n",
    "\n",
    "skel_feat_seqs = seqtools.smap(lambda s: adjust_length(s, max_time), skel_feat_seqs)\n",
    "\n",
    "feat_seqs_train = [\n",
    "    seqtools.gather(skel_feat_seqs, train_subset)\n",
    "    ]\n",
    "transformations_train = seqtools.gather(transformations, train_subset)\n",
    "labels_train = labels[train_subset].astype(np.int32)\n",
    "durations_train = durations[train_subset].astype(np.int32)\n",
    "\n",
    "feat_seqs_val = [\n",
    "    seqtools.gather(skel_feat_seqs, val_subset)\n",
    "    ]\n",
    "transformations_val = seqtools.gather(transformations, val_subset)\n",
    "labels_val = labels[val_subset].astype(np.int32)\n",
    "durations_val = durations[val_subset].astype(np.int32)\n",
    "\n",
    "del transformations, labels, durations, skel_feat_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_triplets(vocabulary, labels, n, test=None):\n",
    "    test = test or (lambda *_: True)\n",
    "    where_labels = {l: np.where(labels == l)[0] for l in vocabulary}\n",
    "    where_not_labels = {l: np.where(labels != l)[0] for l in vocabulary}\n",
    "\n",
    "    triplets = np.empty((n, 3), dtype=np.uint64)\n",
    "\n",
    "    i = 0\n",
    "    while i < len(labels):\n",
    "        left = i % len(labels)\n",
    "        wl = where_labels[labels[left]]\n",
    "        wn = where_not_labels[labels[left]]\n",
    "        middle = np.random.choice(wl)\n",
    "        right = np.random.choice(wn)\n",
    "\n",
    "        while not test(left, middle, right):\n",
    "            middle = np.random.choice(wl)\n",
    "            right = np.random.choice(wn)\n",
    "\n",
    "        triplets[i] = [left, middle, right]\n",
    "        i += 1\n",
    "\n",
    "    return triplets\n",
    "\n",
    "\n",
    "def triplet2minibatches(feat_seqs, labels, durations, transformations):\n",
    "    triplets = np.array(sample_triplets(\n",
    "        sorted(set(labels)), labels, len(labels),\n",
    "        test=lambda i, j, k: transformations[i][0] != transformations[j][0]))\n",
    "    triplets = np.random.permutation(triplets)\n",
    "    feat_triplets = [\n",
    "        seqtools.starmap(\n",
    "            lambda i, j, k: np.stack([f[i], f[j], f[k]], axis=0),\n",
    "            triplets)\n",
    "        for f in feat_seqs]\n",
    "    duration_triplets = seqtools.smap(lambda triplet: durations[triplet], triplets)\n",
    "\n",
    "    feat_batches = [\n",
    "        seqtools.batch(f, batch_size // 3, drop_last=True, collate_fn=np.concatenate)\n",
    "        for f in feat_triplets]\n",
    "    duration_batches = seqtools.batch(\n",
    "        duration_triplets, \n",
    "        batch_size // 3,\n",
    "        drop_last=True, collate_fn=np.concatenate)\n",
    "\n",
    "    return seqtools.collate(feat_batches + [duration_batches])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.siamese_triplet.c_model import skel_rnn\n",
    "\n",
    "batch_size = 12\n",
    "encoder_kwargs = {\n",
    "    \"tconv_sz\": 15,\n",
    "    \"filter_dilation\": 1,\n",
    "    \"num_tc_filters\": 256,\n",
    "    \"dropout\": 0.2\n",
    "}\n",
    "max_time = 128\n",
    "\n",
    "assert batch_size % 3 == 0, \"the model must take triplets\"\n",
    "model_dict = skel_rnn(\n",
    "    *tuple(f[0][0].shape for f in feat_seqs_train), \n",
    "    batch_size=batch_size, max_time=max_time, \n",
    "    encoder_kwargs=encoder_kwargs)\n",
    "l_linout = model_dict['l_linout']\n",
    "l_in = model_dict['l_in']\n",
    "l_duration = model_dict['l_duration']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sltools.models.siamese import triplet_loss\n",
    "\n",
    "l_rate = 1e-4\n",
    "n_epoches = 10\n",
    "\n",
    "l_rate_var = T.scalar('l_rate')\n",
    "linout = lasagne.layers.get_output(l_linout, deterministic=False)\n",
    "train_loss = triplet_loss(linout[0::3], linout[1::3], linout[2::3]).sum()\n",
    "params = lasagne.layers.get_all_params(l_linout, trainable=True)\n",
    "updates = lasagne.updates.adam(train_loss, params, learning_rate=l_rate_var)\n",
    "update_fn = theano.function(\n",
    "    [l.input_var for l in l_in] + [l_duration.input_var, l_rate_var],\n",
    "    outputs=train_loss, updates=updates)\n",
    "\n",
    "linout = lasagne.layers.get_output(l_linout, deterministic=True)\n",
    "test_loss = triplet_loss(linout[0::3], linout[1::3], linout[2::3]).sum()\n",
    "loss_fn = theano.function(\n",
    "    [l.input_var for l in l_in] + [l_duration.input_var],\n",
    "    outputs=test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_train_loss = 10\n",
    "running_val_loss = 10\n",
    "\n",
    "for e in range(10): \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    # Minibatch iterator\n",
    "    train_minibatches = triplet2minibatches(\n",
    "        feat_seqs_train, labels_train, durations_train, transformations_train)\n",
    "    val_minibatches = triplet2minibatches(\n",
    "        feat_seqs_val, labels_val, durations_val, transformations_val)\n",
    "    \n",
    "    # Training iterations\n",
    "    for i, minibatch in enumerate(seqtools.prefetch(train_minibatches, 2, max_buffered=20)):\n",
    "        batch_loss = float(update_fn(*minibatch, l_rate))\n",
    "        if np.isnan(batch_loss):\n",
    "            raise ValueError()\n",
    "        running_train_loss = .99 * running_train_loss + .01 * batch_loss\n",
    "\n",
    "        if i % 30 == 0:\n",
    "            train_losses.append(batch_loss)\n",
    "            batch_losses = [loss_fn(*val_minibatches[j]) \n",
    "                            for j in np.random.choice(len(val_minibatches), 10)]\n",
    "            val_losses.append(batch_losses[-1])\n",
    "            running_val_loss = .91 * running_val_loss + .09 * np.mean(batch_losses)\n",
    "            print(\"\\rloss: {:>2.3f} / {:>2.3f}\".format(running_train_loss, running_val_loss), \n",
    "                  end=\"\", flush=True)\n",
    "\n",
    "    report[str(e)] = {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'epoch_loss': running_train_loss,\n",
    "        'params': lasagne.layers.get_all_param_values(l_linout)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 3))\n",
    "\n",
    "batch_losses = np.concatenate([r['train_losses'] for r in report.values()])\n",
    "\n",
    "x = np.arange(len(batch_losses))\n",
    "y = np.array([np.mean(batch_losses[max(0, i-20):i+20]) for i in range(0, len(batch_losses))])\n",
    "err = np.array([np.std(batch_losses[max(0, i-20):i+20]) for i in range(0, len(batch_losses))])\n",
    "plt.plot(x, y)\n",
    "plt.fill_between(x, y - err, y + err, alpha=.3)\n",
    "\n",
    "batch_losses = np.concatenate([r['val_losses'] for r in report.values()])\n",
    "\n",
    "x = np.arange(len(batch_losses))\n",
    "y = np.array([np.mean(batch_losses[max(0, i-20):i+20]) for i in range(0, len(batch_losses))])\n",
    "err = np.array([np.std(batch_losses[max(0, i-20):i+20]) for i in range(0, len(batch_losses))])\n",
    "plt.plot(x, y)\n",
    "plt.fill_between(x, y - err, y + err, alpha=.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmin(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sltools.models.siamese import build_predict_fn\n",
    "\n",
    "iteration = 9\n",
    "lasagne.layers.set_all_param_values(l_linout, report[str(iteration)]['params'])\n",
    "predict_fn = build_predict_fn(model_dict, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vects = predict_fn(feat_seqs_train + [durations_train])\n",
    "val_vects = predict_fn(feat_seqs_val + [durations_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bisect import bisect_left\n",
    "from scipy.spatial.distance import cdist, pdist, squareform\n",
    "\n",
    "\n",
    "def episode(embeddings, labels, k, n_shots, n_adversaries, n_classes):\n",
    "    vocabulary = np.sort(np.unique(labels))    \n",
    "    voca_subset = np.random.permutation(vocabulary)[:n_classes]\n",
    "    idx_where = {i: np.where(labels == l)[0] for i, l in enumerate(voca_subset)}\n",
    "\n",
    "    ep_embeddings = np.empty([n_shots + 1 + n_adversaries, embeddings.shape[1]])\n",
    "    ep_labels = np.empty([n_shots + 1 + n_adversaries], dtype=np.int)\n",
    "    for i in range(n_adversaries):\n",
    "        l = np.random.randint(n_classes - 1)\n",
    "        m = np.random.choice(idx_where[l])\n",
    "        ep_embeddings[i] = embeddings[m]\n",
    "        ep_labels[i] = l\n",
    "    for i in range(n_adversaries, n_adversaries + n_shots + 1):\n",
    "        l = n_classes - 1\n",
    "        m = np.random.choice(idx_where[l])\n",
    "        ep_embeddings[i] = embeddings[m]\n",
    "        ep_labels[i] = l\n",
    "    \n",
    "    dists = cdist(ep_embeddings[n_adversaries:], ep_embeddings)\n",
    "    neighbours = np.argsort(dists, axis=1)[:, 1:k+1]\n",
    "    counts = np.stack([\n",
    "        np.histogram(ep_labels[n], bins=np.arange(-0.5, n_classes))[0]\n",
    "        for n in neighbours])\n",
    "    ranks = [np.argmax(voca_subset[o[::-1]] == voca_subset[-1])\n",
    "             for o in np.argsort(counts, axis=1)]\n",
    "    return ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1\n",
    "n_shots = 5\n",
    "n_adversaries = 200\n",
    "n_classes = 15\n",
    "\n",
    "train_episodes = []\n",
    "for i in range(1000):\n",
    "    train_episodes.extend(episode(\n",
    "        train_vects, labels_train,\n",
    "        k, n_shots, n_adversaries, n_classes))\n",
    "\n",
    "cutoff = 5\n",
    "plt.figure()\n",
    "ax = plt.subplot(2, 1, 1)\n",
    "bins = list(np.arange(-0.5, cutoff + .5)) + [n_classes]\n",
    "h, _ = np.histogram(train_episodes, bins=bins)\n",
    "ax.bar(np.arange(cutoff + 1), h / h.sum())\n",
    "ax.set_xlim((-.5, cutoff + .5))\n",
    "ax.set_ylim((0, 1))\n",
    "ax.set_title(\"{}-shot\".format(n_shots))\n",
    "ax.set_xlabel(\"rank\")\n",
    "ax.set_xticks([i for i in range(0, cutoff + 1, 2)] + [cutoff])\n",
    "ax.set_xticklabels([i for i in range(0, cutoff + 1, 2)] + [\">{}\".format(cutoff)])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "val_episodes = []\n",
    "for i in range(1000):\n",
    "    val_episodes.extend(episode(\n",
    "        val_vects, labels_val,\n",
    "        k, n_shots, n_adversaries, n_classes))\n",
    "\n",
    "cutoff = 5\n",
    "plt.figure()\n",
    "ax = plt.subplot(2, 1, 2)\n",
    "bins = list(np.arange(-0.5, cutoff + .5)) + [n_classes]\n",
    "h, _ = np.histogram(val_episodes, bins=bins)\n",
    "ax.bar(np.arange(cutoff + 1), h / h.sum())\n",
    "ax.set_xlim((-.5, cutoff + .5))\n",
    "ax.set_ylim((0, 1))\n",
    "ax.set_title(\"{}-shot\".format(n_shots))\n",
    "ax.set_xlabel(\"rank\")\n",
    "ax.set_xticks([i for i in range(0, cutoff + 1, 2)] + [cutoff])\n",
    "ax.set_xticklabels([i for i in range(0, cutoff + 1, 2)] + [\">{}\".format(cutoff)])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
