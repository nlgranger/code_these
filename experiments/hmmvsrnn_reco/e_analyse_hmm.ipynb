{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.normpath(os.path.join(os.getcwd(), '..')))\n",
    "os.environ['THEANO_FLAGS'] = \"device=cuda1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shelve\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import lasagne\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm\n",
    "from lproc import subset, rmap\n",
    "from datasets.utils import gloss2seq, seq2gloss\n",
    "from sltools.nn_utils import onehot, jaccard, compute_scores\n",
    "from sltools.postproc import optimize_boundaries, filter_longshort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reload dataset and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from experiments.ch14_skel.a_data import durations, gloss_seqs, tmpdir, \\\n",
    "#     train_subset, val_subset, test_subset, vocabulary\n",
    "# from experiments.ch14_skel.b_preprocess import feat_seqs\n",
    "# feat_seqs = rmap(lambda x: (x,), feat_seqs)\n",
    "\n",
    "# from experiments.ch14_bgr.a_data import durations, gloss_seqs, tmpdir, \\\n",
    "#     train_subset, val_subset, test_subset, vocabulary\n",
    "# from experiments.ch14_bgr.b_preprocess import feat_seqs\n",
    "# feat_seqs = rmap(lambda x: (x,), feat_seqs)\n",
    "\n",
    "# from experiments.ch14_fusion.a_data import durations, gloss_seqs, tmpdir, \\\n",
    "#     train_subset, val_subset, test_subset, vocabulary\n",
    "# from experiments.ch14_fusion.b_preprocess import feat_seqs\n",
    "\n",
    "from experiments.ch14_transfer2.a_data import durations, gloss_seqs, tmpdir, \\\n",
    "    train_subset, val_subset, test_subset, vocabulary\n",
    "from experiments.ch14_transfer2.b_preprocess import feat_seqs\n",
    "feat_seqs = rmap(lambda x: (x,), feat_seqs)\n",
    "\n",
    "# from experiments.ch14_shorttc.a_data import durations, gloss_seqs, tmpdir, \\\n",
    "#     train_subset, val_subset, test_subset, vocabulary\n",
    "# from experiments.ch14_shorttc.b_preprocess import feat_seqs\n",
    "# feat_seqs = rmap(lambda x: (x,), feat_seqs)\n",
    "\n",
    "feat_seqs_train = subset(feat_seqs, train_subset)\n",
    "gloss_seqs_train = subset(gloss_seqs, train_subset)\n",
    "durations_train = subset(durations, train_subset)\n",
    "\n",
    "feat_seqs_val = subset(feat_seqs, val_subset)\n",
    "gloss_seqs_val = subset(gloss_seqs, val_subset)\n",
    "durations_val = subset(durations, val_subset)\n",
    "\n",
    "feat_seqs_test = subset(feat_seqs, test_subset)\n",
    "gloss_seqs_test = subset(gloss_seqs, test_subset)\n",
    "durations_test = subset(durations, test_subset)\n",
    "\n",
    "report = shelve.open(os.path.join(tmpdir, \"hmm_report\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_batch_losses = []\n",
    "all_epoch_losses = []\n",
    "n_epochs = []\n",
    "phases = list(report.keys())\n",
    "best_phase = phases[int(np.argmax([report[p]['val_report']['jaccard'] for p in phases]))]\n",
    "phase_report = report[str(best_phase)]\n",
    "print(\"best phase is {} with JI {}\".format(best_phase, phase_report['val_report']['jaccard']))\n",
    "recognizer = phase_report['model']\n",
    "previous_recognizer = report[str(int(best_phase) - 1)]['model'] if int(best_phase) > 0 else None\n",
    "\n",
    "pprint(phase_report['settings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 3))\n",
    "plt.scatter(np.arange(len(all_batch_losses)), all_batch_losses, marker='.', alpha=.1)\n",
    "plt.gca().set_yscale(\"log\")\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.plot(np.arange(len(all_epoch_losses)), all_epoch_losses)\n",
    "plt.gca().set_yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_perfs(perf_report, chains_lengths):\n",
    "    plt.figure(figsize=(14, 7))\n",
    "\n",
    "    ax = plt.subplot2grid((2, 3), (0, 0))\n",
    "    cmap = matplotlib.cm.viridis\n",
    "    cmap.set_bad(cmap(0.001))\n",
    "    plt.imshow(perf_report['confusion'] / perf_report['confusion'].sum(axis=1, keepdims=True), \n",
    "               interpolation='none', \n",
    "               cmap=cmap, clim=(0.001, 1), norm=LogNorm(vmin=0.001, vmax=1))\n",
    "    plt.gca().set_xticklabels([])\n",
    "    plt.gca().set_yticklabels([])\n",
    "    ax.set_title(\"HMM Jaccard/Framewise : {:5.4f}/{:5.4f}\".format(\n",
    "        perf_report['jaccard'], perf_report['framewise']))\n",
    "\n",
    "    ax = plt.subplot2grid((2, 3), (1, 0))\n",
    "    cmap = matplotlib.cm.viridis\n",
    "    cmap.set_bad(cmap(0.001))\n",
    "    plt.imshow(perf_report['posterior_confusion'] / perf_report['posterior_confusion'].sum(axis=1, keepdims=True), \n",
    "               interpolation='none', \n",
    "               cmap=cmap, clim=(0.001, 1), norm=LogNorm(vmin=0.001, vmax=1))\n",
    "    plt.gca().set_xticklabels([])\n",
    "    plt.gca().set_yticklabels([])\n",
    "    ax.set_title(\"Posterior Jaccard/Framewise : {:5.4f}/{:5.4f}\".format(\n",
    "        perf_report['posterior_jaccard'], perf_report['posterior_framewise']))\n",
    "\n",
    "    ax = plt.subplot2grid((2, 3), (0, 1), colspan=2, rowspan=2)\n",
    "    cmap = matplotlib.cm.viridis\n",
    "    cmap.set_bad(cmap(0.001))\n",
    "    plt.imshow(perf_report['statewise_confusion'] / perf_report['statewise_confusion'].sum(axis=1, keepdims=True), \n",
    "               interpolation='none', \n",
    "               cmap=cmap, clim=(0.001, 1), norm=LogNorm(vmin=0.001, vmax=1))\n",
    "    plt.gca().set_xticklabels([])\n",
    "    plt.gca().set_yticklabels([])\n",
    "    plt.gca().set_xticks(np.cumsum(chains_lengths) - .5)\n",
    "    plt.gca().set_yticks(np.cumsum(chains_lengths) - .5)\n",
    "    plt.gca().grid(color='gray', linestyle='dotted')\n",
    "    ax.set_title(\"State-wise framewise: {:5.4f}\".format(perf_report['statewise_framewise']))\n",
    "    \n",
    "    plt.colorbar()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_perfs(phase_report['train_report'], recognizer.chains_lengths)\n",
    "plot_perfs(phase_report['val_report'], recognizer.chains_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_train = [gloss2seq(g_, d_, 0) for g_, d_ in zip(gloss_seqs_train, durations_train)]\n",
    "predictions_train = recognizer.predict(feat_seqs_train)\n",
    "ji_train, framewise_train, confusion_train = compute_scores(predictions_train, targets_train, vocabulary)\n",
    "\n",
    "targets_val = [gloss2seq(g_, d_, 0) for g_, d_ in zip(gloss_seqs_val, durations_val)]\n",
    "predictions_val = recognizer.predict(feat_seqs_val)\n",
    "ji_val, framewise_val, confusion_val = compute_scores(predictions_val, targets_val, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of errors\n",
    "\n",
    "scores = [jaccard(onehot(l, vocabulary), onehot(p, vocabulary))\n",
    "         for l, p in zip(targets_val, predictions_val)]\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(scores, np.linspace(0.5, 1, 40))\n",
    "plt.title(\"Histogram of sequence-wise JI\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb of false positives out of sequence vocabulary\n",
    "np.mean([len(set(p_) - set(l_)) for p_, l_ in zip(predictions_val, targets_val)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion types\n",
    "cum_err = np.sum(confusion_val, axis=1) - np.diag(confusion_val)\n",
    "print(\"false pos: {}  false neg: {}, mis-class: {}\".format(\n",
    "    cum_err[0], np.sum(confusion_val[1:, 0]), np.sum(cum_err[1:]) - np.sum(confusion_val[1:, 0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the posteriors of the _correct_ states in color and other states in gray\n",
    "\n",
    "def preview_seq(proba, gloss):\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    cmap = plt.cm.summer(np.linspace(0, 1, 5))\n",
    "    \n",
    "    pre_start = 0\n",
    "    for lbl, start, stop in gloss:\n",
    "        for i in range(proba.shape[1] - 1):\n",
    "            plt.plot(np.arange(pre_start, start), proba[pre_start:start, i], ls=':', c=\"gray\")\n",
    "        plt.plot(np.arange(pre_start, start), proba[pre_start:start, -1], c=\"purple\")\n",
    "        for a in range(0, (lbl - 1) * 5):\n",
    "            plt.plot(np.arange(start, stop), proba[start:stop, a], ls=\":\", c='gray')\n",
    "        for a in range(5):\n",
    "            plt.plot(np.arange(start, stop), proba[start:stop, (lbl - 1) * 5 + a], c=cmap[a])\n",
    "        for a in range(lbl * 5, proba.shape[1]):\n",
    "            plt.plot(np.arange(start, stop), proba[start:stop, a], ls=\":\", c='gray')\n",
    "        pre_start = stop\n",
    "    \n",
    "    for i in range(proba.shape[1] - 1):\n",
    "        plt.plot(np.arange(pre_start, len(proba)), proba[pre_start:len(proba), i], ls=':', c=\"gray\")\n",
    "    plt.plot(np.arange(pre_start, len(proba)), proba[pre_start:len(proba), -1], c=\"purple\")\n",
    "\n",
    "    \n",
    "seq = 15\n",
    "\n",
    "preview_seq(\n",
    "    recognizer.posterior.predict_proba(feat_seqs_val[seq][0]),\n",
    "    gloss_seqs_val[seq])\n",
    "\n",
    "plt.gca().set_xlim((200, 900))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlate error with predicted gloss duration\n",
    "\n",
    "plt.figure(figsize=(9, 4))\n",
    "\n",
    "prediction_accuracy = [np.sum(l[start:stop] == g) \n",
    "            for p, l in zip(predictions_val, targets_val)\n",
    "            for (g, start, stop) in seq2gloss(p)\n",
    "            if g != 0]\n",
    "none_accuracy = [np.sum(l[start:stop] == 0)\n",
    "            for p, l in zip(predictions_val, targets_val)\n",
    "            for (g, start, stop) in seq2gloss(p)\n",
    "            if g != 0]\n",
    "gloss_d = [stop - start\n",
    "           for p in predictions_val \n",
    "           for (g, start, stop) in seq2gloss(p)\n",
    "           if g != 0]\n",
    "\n",
    "scores_pred = np.zeros((int(np.ceil(max(gloss_d) / 5 + 0.0001)),))\n",
    "scores_none = np.zeros((int(np.ceil(max(gloss_d) / 5 + 0.0001)),))\n",
    "total_d = np.zeros((int(np.ceil(max(gloss_d) / 5 + 0.0001)),))\n",
    "for vp, vn, d in zip(prediction_accuracy, none_accuracy, gloss_d):\n",
    "    idx = int(d / 5)\n",
    "    scores_pred[idx] += vp\n",
    "    scores_none[idx] += vn\n",
    "    total_d[idx] += d\n",
    "\n",
    "plt.gca().bar(np.arange(0, int(np.ceil(max(gloss_d) + 0.0001)), 5), \n",
    "              scores_pred / total_d,\n",
    "              width=5,\n",
    "              alpha=.5)\n",
    "plt.gca().bar(np.arange(0, int(np.ceil(max(gloss_d) + 0.0001)), 5), \n",
    "              scores_none / total_d,\n",
    "              width=5,\n",
    "              alpha=.5)\n",
    "\n",
    "plt.legend([\"predicted class\", \"non-gesture class\"])\n",
    "plt.xlabel(\"subsequence duration (based on model prediction)\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize duration filter\n",
    "\n",
    "boundaries = optimize_boundaries(targets_val, predictions_val, vocabulary, (30, 90, 300))\n",
    "print(\"Optimal range: \", boundaries)\n",
    "print(\"FYI, validation score without is : {:.4f}\".format(ji_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation score\n",
    "\n",
    "ji_after, _, _ = compute_scores(\n",
    "    [filter_longshort(p, boundaries, 0) for p in predictions_val], \n",
    "    targets_val, vocabulary)\n",
    "print(\"validation score: {:.4f}\".format(ji_after))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test score\n",
    "\n",
    "targets_test = [gloss2seq(g_, d_, 0) for g_, d_ in zip(gloss_seqs_test, durations_test)]\n",
    "predictions_test = recognizer.predict(feat_seqs_test)\n",
    "\n",
    "ji_test, _, _ = compute_scores(\n",
    "    [filter_longshort(p, boundaries, 0) for p in predictions_test], \n",
    "    targets_test, vocabulary)\n",
    "print(\"testing score: {:.4f}\".format(ji_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(recognizer.posterior.build_encoder.source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sltools.tconv import TemporalConv\n",
    "\n",
    "l = None\n",
    "for l_ in lasagne.layers.get_all_layers(recognizer.posterior.l_feats):\n",
    "    if isinstance(l_, TemporalConv):\n",
    "        l = l_\n",
    "\n",
    "W1 = l.W.eval()\n",
    "W1 = W1.transpose((0, 2, 1)).reshape((-1, W1.shape[1]))\n",
    "Y = np.linalg.norm(W1, axis=1)\n",
    "i = np.argsort(Y)\n",
    "W1 = np.stack([W1[i_] for i_ in i])[-300:]\n",
    "model = TSNE(n_components=1, metric='euclidean')\n",
    "Y = model.fit_transform(W1)[:, 0]\n",
    "i = np.argsort(Y)\n",
    "W1 = W1[i]\n",
    "Y = Y[i]\n",
    "\n",
    "plt.figure(figsize=(5, 20))\n",
    "x, y = np.meshgrid(np.arange(W1.shape[1]), Y)\n",
    "plt.imshow(W1, clim=(-np.abs(W1).max(), np.abs(W1).max()), cmap='bwr')\n",
    "plt.gca().set_aspect(\"auto\")\n",
    "plt.axis([-1, W1.shape[1], -1, W1.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l = lasagne.layers.get_all_layers(recognizer.posterior.l_feats)[2]\n",
    "# W = np.asarray(l.W.eval())\n",
    "\n",
    "# nrows, ncols = int(np.ceil(np.sqrt(W.shape[0] + 1))), int(np.floor(np.sqrt(W.shape[0] + 1)))\n",
    "# img = np.zeros((nrows * (W.shape[2] + 1), ncols * (W.shape[3] + 1)))\n",
    "# for k in range(W.shape[0]):\n",
    "#     i, j = k // ncols, k % ncols\n",
    "#     y, x = i * (W.shape[2] + 1), j * (W.shape[3] + 1)\n",
    "#     tmp = img[y:y + W.shape[2]]\n",
    "#     img[y:y + W.shape[2], x:x + W.shape[3]] = W[k, 0]\n",
    "\n",
    "# plt.imshow(img, clim=(-np.abs(W).max(), np.abs(W).max()), cmap='bwr')\n",
    "# plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl, cnt = np.unique(\n",
    "    np.concatenate([gloss2seq(g_, len(r_), 0) for g_, r_ in zip(gloss_seqs_val, feat_seqs_val)]),\n",
    "    return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(x=lbl+.5, height=cnt, log=True)\n",
    "plt.gca().set_ylim((1, 3e5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(x=lbl+.5, height=cnt, log=False)\n",
    "plt.gca().set_ylim((1, 2e5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([len(gseq) for gseq in gloss_seqs_val])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
