{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "\n",
    "os.environ['THEANO_FLAGS'] = \"device=cuda1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shelve\n",
    "import collections\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "import seqtools\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "from sltools.nn_utils import adjust_length\n",
    "from sltools.models.siamese import build_predict_fn\n",
    "\n",
    "from experiments.siamese_triplet.a_data import \\\n",
    "    durations, labels, recordings, \\\n",
    "    train_subset, val_subset\n",
    "from experiments.siamese_triplet.b_preprocess import skel_feat_seqs\n",
    "\n",
    "np.set_printoptions(linewidth=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.siamese_triplet.a_data import cachedir\n",
    "\n",
    "report = shelve.open(os.path.join(cachedir, \"rnn_report\"))\n",
    "report.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_seqs_train = [\n",
    "    seqtools.gather(skel_feat_seqs, train_subset)\n",
    "    ]\n",
    "recordings_train = seqtools.gather(recordings, train_subset)\n",
    "labels_train = labels[train_subset].astype(np.int32)\n",
    "durations_train = durations[train_subset].astype(np.int32)\n",
    "\n",
    "feat_seqs_val = [\n",
    "    seqtools.gather(skel_feat_seqs, val_subset)\n",
    "    ]\n",
    "labels_val = labels[val_subset].astype(np.int32)\n",
    "durations_val = durations[val_subset].astype(np.int32)\n",
    "\n",
    "del recordings, labels, durations, skel_feat_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_triplets(vocabulary, labels, n, test=None):\n",
    "    test = test or (lambda *_: True)\n",
    "    where_labels = {l: np.where(labels == l)[0] for l in vocabulary}\n",
    "    where_not_labels = {l: np.where(labels != l)[0] for l in vocabulary}\n",
    "\n",
    "    triplets = np.empty((n, 3), dtype=np.uint64)\n",
    "\n",
    "    i = 0\n",
    "    while i < len(labels):\n",
    "        left = i % len(labels)\n",
    "        wl = where_labels[labels[left]]\n",
    "        wn = where_not_labels[labels[left]]\n",
    "        middle = np.random.choice(wl)\n",
    "        right = np.random.choice(wn)\n",
    "\n",
    "        while not test(left, middle, right):\n",
    "            middle = np.random.choice(wl)\n",
    "            right = np.random.choice(wn)\n",
    "\n",
    "        triplets[i] = [left, middle, right]\n",
    "        i += 1\n",
    "\n",
    "    return np.random.permutation(triplets)\n",
    "\n",
    "\n",
    "def triplet2minibatches(feat_seqs, durations, triplets):\n",
    "    feat_seqs = [seqtools.smap(lambda s: adjust_length(s, max_time), f) \n",
    "                 for f in feat_seqs]\n",
    "    durations = np.fmin(durations, max_time)\n",
    "\n",
    "    feat_triplets = [\n",
    "        seqtools.starmap(\n",
    "            lambda i, j, k: np.stack([\n",
    "                adjust_length(f[i], max_time), \n",
    "                adjust_length(f[j], max_time), \n",
    "                adjust_length(f[k], max_time)], axis=0),\n",
    "            triplets)\n",
    "        for f in feat_seqs]\n",
    "    duration_triplets = seqtools.smap(lambda triplet: durations[triplet], triplets)\n",
    "\n",
    "    feat_batches = [\n",
    "        seqtools.batch(f, batch_size // 3, drop_last=True, collate_fn=np.concatenate)\n",
    "        for f in feat_triplets]\n",
    "    duration_batches = seqtools.batch(\n",
    "        duration_triplets, \n",
    "        batch_size // 3,\n",
    "        drop_last=True, collate_fn=np.concatenate)\n",
    "\n",
    "    return seqtools.collate(feat_batches + [duration_batches])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.siamese_triplet.c_model import skel_rnn\n",
    "\n",
    "max_time = 128\n",
    "batch_size = 12\n",
    "encoder_kwargs = {\n",
    "    \"tconv_sz\": 15,\n",
    "    \"filter_dilation\": 1,\n",
    "    \"num_tc_filters\": 256,\n",
    "    \"dropout\": 0.2\n",
    "}\n",
    "\n",
    "assert batch_size % 3 == 0, \"the model must take triplets\"\n",
    "\n",
    "model_dict = skel_rnn(\n",
    "    *tuple(f[0][0].shape for f in feat_seqs_train), \n",
    "    batch_size=batch_size, max_time=max_time, \n",
    "    encoder_kwargs=encoder_kwargs)\n",
    "\n",
    "l_linout = model_dict['l_linout']\n",
    "l_in = model_dict['l_in']\n",
    "l_duration = model_dict['l_duration']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sltools.models.siamese import triplet_loss\n",
    "\n",
    "l_rate = 1e-4\n",
    "n_epoches = 10\n",
    "\n",
    "l_rate_var = T.scalar('l_rate')\n",
    "linout = lasagne.layers.get_output(l_linout, deterministic=False)\n",
    "train_loss = triplet_loss(linout[0::3], linout[1::3], linout[2::3]).sum()\n",
    "params = lasagne.layers.get_all_params(l_linout, trainable=True)\n",
    "updates = lasagne.updates.adam(train_loss, params, learning_rate=l_rate_var)\n",
    "update_fn = theano.function(\n",
    "    [l.input_var for l in l_in] + [l_duration.input_var, l_rate_var],\n",
    "    outputs=train_loss, updates=updates)\n",
    "\n",
    "linout = lasagne.layers.get_output(l_linout, deterministic=True)\n",
    "test_loss = triplet_loss(linout[0::3], linout[1::3], linout[2::3]).sum()\n",
    "loss_fn = theano.function(\n",
    "    [l.input_var for l in l_in] + [l_duration.input_var],\n",
    "    outputs=test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_train_loss = 2\n",
    "running_val_loss = 2\n",
    "\n",
    "for e in range(3, 6): \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    # Minibatch iterator\n",
    "    triplets = sample_triplets(\n",
    "        sorted(set(labels_train)), labels_train, len(labels_train),\n",
    "        test=lambda i, j, k: recordings_train[i] != recordings_train[j])\n",
    "    train_minibatches = triplet2minibatches(\n",
    "        feat_seqs_train, durations_train, triplets)\n",
    "    triplets = sample_triplets(\n",
    "        sorted(set(labels_val)), labels_val, len(labels_val),\n",
    "        test=lambda i, j, k: True)\n",
    "    val_minibatches = triplet2minibatches(\n",
    "        feat_seqs_val, durations_val, triplets)\n",
    "    \n",
    "    # Training iterations\n",
    "    for i, minibatch in enumerate(seqtools.prefetch(train_minibatches, 2, max_buffered=20)):\n",
    "        batch_loss = float(update_fn(*minibatch, l_rate))\n",
    "        if np.isnan(batch_loss):\n",
    "            raise ValueError()\n",
    "        running_train_loss = .99 * running_train_loss + .01 * batch_loss\n",
    "\n",
    "        if i % 3 == 0:\n",
    "            train_losses.append(batch_loss)\n",
    "        \n",
    "        if i % 30 == 0:\n",
    "            batch_losses = [loss_fn(*val_minibatches[j]) \n",
    "                            for j in np.random.choice(len(val_minibatches), 10)]\n",
    "            val_losses.extend(batch_losses)\n",
    "            running_val_loss = .91 * running_val_loss + .09 * np.mean(batch_losses)\n",
    "            print(\"\\rloss: {:>2.3f} / {:>2.3f}\".format(running_train_loss, running_val_loss), \n",
    "                  end=\"\", flush=True)\n",
    "\n",
    "    l_rate *= 0.3\n",
    "            \n",
    "    report[str(e)] = {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'epoch_loss': running_train_loss,\n",
    "        'params': lasagne.layers.get_all_param_values(l_linout)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "batch_losses = np.concatenate([r['train_losses'] for r in report.values()])\n",
    "\n",
    "x = np.arange(len(batch_losses))\n",
    "y = np.array([np.mean(batch_losses[max(0, i-50):i+50]) for i in range(0, len(batch_losses))])\n",
    "err = np.array([np.std(batch_losses[max(0, i-50):i+50]) for i in range(0, len(batch_losses))])\n",
    "plt.plot(x, y)\n",
    "plt.fill_between(x, y - err, y + err, alpha=.3)\n",
    "\n",
    "batch_losses = np.concatenate([r['val_losses'] for r in report.values()])\n",
    "\n",
    "x = np.arange(len(batch_losses))\n",
    "y = np.array([np.mean(batch_losses[max(0, i-50):i+50]) for i in range(0, len(batch_losses))])\n",
    "err = np.array([np.std(batch_losses[max(0, i-50):i+50]) for i in range(0, len(batch_losses))])\n",
    "plt.plot(x, y)\n",
    "plt.fill_between(x, y - err, y + err, alpha=.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iteration = 9\n",
    "# lasagne.layers.set_all_param_values(l_linout, report[str(iteration)]['params'])\n",
    "\n",
    "predict_fn = build_predict_fn(model_dict, batch_size, max_time)\n",
    "embeddings_train = predict_fn(feat_seqs_train, durations_train)\n",
    "embeddings_val = predict_fn(feat_seqs_val, durations_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def episode(embeddings, labels, voca_size, shots, k):    \n",
    "    vocabulary = np.sort(np.unique(labels))\n",
    "    \n",
    "    # sample vocabulary subset\n",
    "    ep_vocabulary = np.random.choice(vocabulary, size=voca_size, replace=False)\n",
    "    \n",
    "    ep_train_subset = []\n",
    "    ep_test_subset = []\n",
    "    for l in ep_vocabulary:\n",
    "        where_label = np.random.permutation(np.where(labels == l)[0])\n",
    "        ep_train_subset.extend(where_label[:shots])\n",
    "        ep_test_subset.extend(where_label[shots:])\n",
    "\n",
    "    # run knn\n",
    "    dists = cdist(embeddings[ep_test_subset], embeddings[ep_train_subset])\n",
    "#     plt.figure()\n",
    "#     plt.imshow(dists)\n",
    "#     plt.show()\n",
    "    \n",
    "    neighbours = np.argsort(dists, axis=1)[:, :k]\n",
    "    \n",
    "    neighbours_labels = labels[ep_train_subset][None, neighbours][0]\n",
    "    neighbours_dists = dists[np.arange(len(ep_test_subset))[:, None], neighbours]\n",
    "    \n",
    "    stats = np.empty((len(ep_test_subset), voca_size), dtype=[('freq', 'i4'), ('dist_score', 'f4'), ('class', 'i4')])\n",
    "    for i, l in enumerate(ep_vocabulary):\n",
    "        stats['freq'][:, i] = np.sum(neighbours_labels == l, axis=1)\n",
    "        stats['dist_score'][:, i] = -np.sum(neighbours_dists * (neighbours_labels == l), axis=1)\n",
    "        stats['class'][:, i] = l\n",
    "    \n",
    "    stats = np.sort(stats, axis=1)\n",
    "    \n",
    "    ranks = voca_size - 1 - np.argmax(labels[ep_test_subset, None] == stats['class'], axis=1)\n",
    "    \n",
    "    return ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbours = 1  # k\n",
    "shots = 3\n",
    "voca_size = 20\n",
    "\n",
    "train_ranks = []\n",
    "train_dists = []\n",
    "train_stats = []\n",
    "for i in range(50):\n",
    "    _, unique_indices = np.unique(recordings_train, return_index=True)\n",
    "    \n",
    "    ranks = episode(\n",
    "        embeddings_train[unique_indices], labels_train[unique_indices],\n",
    "        voca_size, shots, neighbours)\n",
    "    train_ranks.extend(ranks)\n",
    "\n",
    "cutoff = neighbours + 2\n",
    "plt.figure()\n",
    "ax = plt.subplot(2, 1, 1)\n",
    "bins = list(np.arange(-0.5, cutoff + .5)) + [voca_size]\n",
    "h, _ = np.histogram(train_ranks, bins=bins)\n",
    "ax.bar(np.arange(cutoff + 1), h / h.sum())\n",
    "ax.set_xlim((-.5, cutoff + .5))\n",
    "ax.set_ylim((0, 1))\n",
    "ax.set_title(\"{}-shot\".format(shots))\n",
    "ax.set_xlabel(\"rank\")\n",
    "ax.set_xticks([i for i in range(0, cutoff + 1, 2)] + [cutoff])\n",
    "ax.set_xticklabels([i for i in range(0, cutoff + 1, 2)] + [\">{}\".format(cutoff)])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "val_ranks = []\n",
    "val_dists = []\n",
    "for i in range(50):\n",
    "    ranks = episode(\n",
    "        embeddings_val, labels_val,\n",
    "        voca_size, shots, neighbours)\n",
    "    val_ranks.extend(ranks)\n",
    "\n",
    "cutoff = neighbours + 2\n",
    "plt.figure()\n",
    "ax = plt.subplot(2, 1, 1)\n",
    "bins = list(np.arange(-0.5, cutoff + .5)) + [voca_size]\n",
    "h, _ = np.histogram(val_ranks, bins=bins)\n",
    "ax.bar(np.arange(cutoff + 1), h / h.sum())\n",
    "ax.set_xlim((-.5, cutoff + .5))\n",
    "ax.set_ylim((0, 1))\n",
    "ax.set_title(\"{}-shot\".format(shots))\n",
    "ax.set_xlabel(\"rank\")\n",
    "ax.set_xticks([i for i in range(0, cutoff + 1, 2)] + [cutoff])\n",
    "ax.set_xticklabels([i for i in range(0, cutoff + 1, 2)] + [\">{}\".format(cutoff)])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
